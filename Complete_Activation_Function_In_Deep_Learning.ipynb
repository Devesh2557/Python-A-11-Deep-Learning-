{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpSqNJnOnrdKGqc8J5WN8S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Devesh2557/Python-A-11-Deep-Learning-/blob/main/Complete_Activation_Function_In_Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Activation Function"
      ],
      "metadata": {
        "id": "rWLVSCkDrKxa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Activation function are mathematical function applied to the outputs of individual neurons in a neural network. They introduce non-linearity into the network, allowing it to learn and approximate complex relationships between inputs and outputs.\n",
        "\n",
        "some commonly used activation functions in deep learning:-\n",
        "\n",
        "(1). Sigmoid Function(Logistic function): It maps the input to a value between 0 and 1. It was widely used in the past but is now less popular due to some drawbacks such as vanishing gradients.\n",
        "\n",
        "(2). Hyperbolic tangent function(tanh): Similar to the sigmoid function but it maps the input to a value between -1 and 1. It is still is used in some cased, but it also suffers from vanishing gradients.\n",
        "\n",
        "(3). Rectified Linear Unit(relu): This function sets all negative values to zero and keeps positive values unchanged. It is the most popular activation function in deep learning, due to it's simplicity and effectiveness in training deep neural networks.\n",
        "\n",
        "(4). Leaky RELU: This function is similar to RELU but allows a small negative slope for negative input values. It helps mitigate the 'dying relu' problem where some neurons can became permanentaly inactive during training.\n",
        "\n",
        "(5). Parametric RELU(PRELU): PRELU is a generalization of relu the introduce a learnable parameter to determine the slope of negative input values. It offers more flexibility and can improve model performance.\n",
        "\n",
        "(6). Exponential Linear Unit(ELU): ELU is a variation of relu that allows negative values with a smooth exponential decay. It helps alleviate the dying relu problem and can produce more robust models.\n",
        "\n",
        "(7). Softmax: Softmax is commonly used in the output layer of a neural network for multi-class classification problem. It normalizes the output values to represent probabilities, ensuring that the sum of all probabilities is 1."
      ],
      "metadata": {
        "id": "dLqHxyY1rQVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In which scenerio which type of optimizer you will use (sgd, momentum, nag, adagard, rmsprop, adam)"
      ],
      "metadata": {
        "id": "_DCwThS0xEgK"
      },
      "execution_count": 1,
      "outputs": []
    }
  ]
}