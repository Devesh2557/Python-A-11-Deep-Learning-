{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOsnM78nOQQu82ZMdBSwn6g"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "Lf7H1Ar5uexu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. prefix - character(s) at the begining = $(\".)\n",
        "# 2. suffix - character(s) at the end = km).!\"\n",
        "# 3. Infix character(s) in between = --/... -\n",
        "# 4. Exception Special-case rule to split a string into several tokens or\n",
        "#    prevent a token from being split when punctuation rules are applied = Let's"
      ],
      "metadata": {
        "id": "0-xj-Tq3uiIk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Approach the tokenization using the split function\n",
        "# 1. word tokenization\n",
        "sent1 = \"I am going to delhi\"\n",
        "sent1.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjvpDoy1uyg3",
        "outputId": "14b824b8-2ae4-4d36-b079-6f26df74289d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'am', 'going', 'to', 'delhi']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sentence tokenization\n",
        "sent2 = \"I am going to delhi. I will stay there for 3 days. Let\\'s hope the trip to be great\"\n",
        "sent2.split('.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icNTlxqru5IG",
        "outputId": "c0243daf-9604-4997-931d-d65a2a7d3be9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I am going to delhi',\n",
              " ' I will stay there for 3 days',\n",
              " \" Let's hope the trip to be great\"]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Problems with split function\n",
        "sent3 = \"I am going to delhi!\"\n",
        "sent3.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNhalYqlv6J3",
        "outputId": "fafc4c54-c117-4936-a9f8-8e2bfd03441f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'am', 'going', 'to', 'delhi!']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# in the sent3 problem, there is seeing that! is include with delhi. SO, we so, we have to remove it by r"
      ],
      "metadata": {
        "id": "oL3QBsD4wHVL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "sent3 = \"I am going to delhi\"\n",
        "tokens = re.findall(\"[\\w]+\", sent3)\n",
        "sent3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "bQ9FXrcrwTxy",
        "outputId": "edbdadd1-669d-4269-fde7-c63208f10096"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:4: SyntaxWarning: invalid escape sequence '\\w'\n",
            "<>:4: SyntaxWarning: invalid escape sequence '\\w'\n",
            "/tmp/ipython-input-2911055583.py:4: SyntaxWarning: invalid escape sequence '\\w'\n",
            "  tokens = re.findall(\"[\\w]+\", sent3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I am going to delhi'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. NLTK"
      ],
      "metadata": {
        "id": "c4Xz-b31wwOo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ],
      "metadata": {
        "id": "V1Js-AFnw2UW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHEJMQP6w71q",
        "outputId": "4b5df979-d347-4589-d2a9-003a23e7b28b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ps5Crs7Ww_0Y",
        "outputId": "e62e305f-a9e9-4d46-97e8-ba1b9cddff8c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent1 = \"I am going to visit delhi\"\n",
        "word_tokenize(sent1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InbkGif4xJ0B",
        "outputId": "cb270374-e1b8-4f0a-fec2-eddfe3c76e86"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'am', 'going', 'to', 'visit', 'delhi']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent2 = 'I have a Ph.D. in A.I.'\n",
        "sent3 = \"We 're here to help! mail us at nks@gmail.com \"\n",
        "sent4 = 'A 5km ride cost $10.50'"
      ],
      "metadata": {
        "id": "llwCLkIQxPiy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(sent2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-bdEd1Ixe0I",
        "outputId": "a39c6cd5-a882-4569-b655-e1c2c9df3981"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'have', 'a', 'Ph.D.', 'in', 'A.I', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  stemming"
      ],
      "metadata": {
        "id": "_aeeIc3NxgbI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Inflection ==> \"In grammar inflection is the modification of a word\n",
        "#                   to express different grametical categories such as tense, case voice\n",
        "#                   aspect person, number gender and mood.\"\n",
        "# 2. Stemming ==> Stemming is the process of reducing inflection in words to\n",
        "#                 their root forms such as mapping a group of words to the same stem even if the\n",
        "#                 stem itslef is not valid word in the Language"
      ],
      "metadata": {
        "id": "SGhmJZFRx6X8"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import PorterStemmer"
      ],
      "metadata": {
        "id": "Se0YlkdWyEf8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ps = PorterStemmer()\n",
        "\n",
        "def stem_words (text):\n",
        "  return \" \".join([ps.stem(word) for word in text.split()])"
      ],
      "metadata": {
        "id": "nfBn0VCCyKUP"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = \"walk walks walking walked\"\n",
        "stem_words(sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "yAdIF88IyZGr",
        "outputId": "b42432a8-ccc3-466c-ae0c-b65d25cc8e47"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'walk walk walk walk'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# stemming mapping a group pf words to the same stem even if the stem itself\n",
        "# is not valid word in Language. So this is the big issue of stemming .\n",
        "text = 'probably my alltime favourite movie a story of selflessness sacrifice and dedication.'\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cN4LuZWy7xi",
        "outputId": "01f26c31-717d-4df3-980e-122c0237d136"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probably my alltime favourite movie a story of selflessness sacrifice and dedication.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stem_words(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "tQxSBNn3znKC",
        "outputId": "e5a9d625-208a-4c6f-baca-4a38efe934cf"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'probabl my alltim favourit movi a stori of selfless sacrific and dedication.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# So , we have to use lemmatization instead of stemming.\n",
        "\n",
        "# note - stemming is fast and Lemmatization is slow"
      ],
      "metadata": {
        "id": "8B2yiaqFzyTS"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization unlike Stemming, reduces the inflected words properly\n",
        "# ensuring that\n",
        "# the root word belongs to the Language.\n",
        "# In Lemitization root word is called Lemma.\n",
        "# Alemma (plural Lemmas or Lemmata) is the canonical form, dictionary form, or\n",
        "# citation form of a set of words\n",
        "\n",
        "# Stemming works on beehalf algorithm so it is fast and Lemmatization is using\n",
        "# wordnet package so it's slow."
      ],
      "metadata": {
        "id": "I2g8ArfN0JR5"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjHyB-QW2K46",
        "outputId": "6c389bef-b6ad-4725-a286-3d0d6354d11c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordnet_lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "4P70BDN32XF8"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'He was running and eating at the same time He has bad habbit of swimming after playing long hours in the sun'\n",
        "punctuation = '?:!.,;'\n",
        "sentence_words = nltk.word_tokenize(sentence)\n",
        "for word in sentence_words:\n",
        "  sentence_words.remove(word)\n",
        "\n",
        "sentence_words\n",
        "print(\"{0:20}{1:20}\".format(\"Word\", \"Lemma\"))\n",
        "for word in sentence_words:\n",
        "  print(\"{0:20}{1:20}\".format(word, wordnet_lemmatizer.lemmatize(word)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jtm2KKr3DlJ",
        "outputId": "22b44344-02cc-4a25-b416-bc0a2dc4b6e8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word                Lemma               \n",
            "was                 wa                  \n",
            "and                 and                 \n",
            "at                  at                  \n",
            "same                same                \n",
            "He                  He                  \n",
            "bad                 bad                 \n",
            "of                  of                  \n",
            "after               after               \n",
            "long                long                \n",
            "in                  in                  \n",
            "sun                 sun                 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JEcZ4M_X3bNj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}