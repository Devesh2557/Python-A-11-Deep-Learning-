{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPm4MtJ/G0duV2Nd6UkSp3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# An optimizer refers to an algorithm or method that adjusts the parameters of a neural network during the training process in order to minimize the loss function and improve the model's performance. The primary goal of optimization in deep learning is to find the optimal set of parameters that allow the neural network to make accurate predictions on new, unseen data."
      ],
      "metadata": {
        "id": "RL_zu76m4tSi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Total Optimizers==>\n",
        "\n",
        "In deep learning, optimizers are algorithms used to update the parameters of a neural network model during the training process. They play a crucial role in minimizing the loss function and improving the accuracy of the model. Here are several popular optimizers along with their explanations:\n",
        "\n",
        "\n",
        "(1). Stochastic Gradient Descent (SGD):\n",
        "\n",
        "SGD is one of the simplest optimizers. It updates the parameters in the direction of the negative gradient of the loss function with respect to the current mini-batch of training data. However, SGD has a limitation of converging slowly and can get stuck in local minima.\n",
        "\n",
        "(2). Momentum:\n",
        "\n",
        "The Momentum optimizer builds upon SGD by adding a momentum term. It accumulates a fraction of the previous gradients to determine the direction of the update. This helps to accelerate convergence and navigate through flat regions and local minima.\n",
        "\n",
        "(3). Nesterov Accelerated Gradient (NAG):\n",
        "\n",
        "NAG is an improvement over the Momentum optimizer. It calculates the gradient not only based on the current parameters but also using an estimate of the future parameters. By looking ahead, NAG allows the optimizer to better anticipate the momentum's effect and adjust its update accordingly.\n",
        "\n",
        "(4). AdaGrad:\n",
        "\n",
        "AdaGrad adapts the learning rate for each parameter based on the historical gradients. It increases the learning rate for infrequent features and decreases it for frequent ones. This makes AdaGrad well-suited for sparse data but can cause the learning rate to become too small over time.\n",
        "\n",
        "\n",
        "(5). RMSprop:\n",
        "\n",
        "RMSprop addresses the diminishing learning rate issue of AdaGrad by introducing an exponentially decaying average of squared gradients. By keeping a moving average of the squared gradients, RMSprop normalizes the learning rates and improve convergence.\n",
        "\n",
        "(6). Adam (Adaptive Moment Estimation):\n",
        "\n",
        "Adam combines the concepts of momentum and RMSprop. It maintains an exponentially decaying average of past gradients and squared gradients. Adam uses bias correction to account for the initialization bias in the first few iterations, making it perform well in practice across different deep learning tasks."
      ],
      "metadata": {
        "id": "4Cc8AO4K5FkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UlVj5FUu5K3b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}